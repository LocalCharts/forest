\date{2024-04-30}
\author{eigil-rischel}
\title{Convex Duality made Difficult}
\import{macros}
%Introduction
\subtree{
  \title{Introduction}
  \p{
    The study of convex functions - in particular, of their optimization (really \em{minimization}) is one of the most important fields of applied mathematics. Convexity seems to be one of those incredibly well-chosen hypotheses which is just specific enough to admit a wealth of theorems, just general enough to produce a nontrivial theory (and a large amount of important examples).
  }
  \p{
    Convex optimization, possibly because it has an "analytical" rather than "algebraic" feel, has not been very thoroughly studied by applied category theorists. The one notable exception is \ref{hanks-etal-convex-2024}, which studies the decomposition of optimization problems by categorical means. This paper takes a different approach, attempting to define a category with optimization problems as the objects, and to derive theorems about optimization by categorical means.
  }
}
%Convex optimization problems
\subtree{
  \title{Convex optimization}

  %def: std. form convex
  \transclude{efr-001G}

  %Def: Lagrangian
  \transclude{efr-002K}
  \p{
    Observe that #{\sup_{\lambda,\nu} L(x,\lambda,\nu)} is #{f_0(x)} if #{x} satisfies the constraints of the problem, and #{\infty} otherwise. Hence we can think of this minimization problem as playing a zero-sum game: we choose #{x}, our adversary chooses #{\lambda,\nu}, and our loss function is #{L}.
  }
  \p{
    It is natural to ask about the existence of Nash equilibria in this game - observe that the existence of an equilibrium #{(x^*,\lambda^*,\nu^*)} means that #{\inf_x \sup_{\lambda,\nu} L(x,\lambda,\nu) = \sup_{\lambda,\nu} \inf_x L(x,\lambda,\nu) = L(x^*,\lambda^*,\nu^*)}. This is of great utility in solving the original problem.
  }
  \p{
    The \em{dual problem} is the problem of \em{maximizing} the function #{\inf_x L(x,\lambda,\nu)}. This is always a concave problem.
  }
  \p{
    In the world of convex optimization, two problems whose constraints carve out the same subset of #{\RR^k} (and where the function to optimize is the same) would be called \em{equivalent}. But they can clearly not be regarded as \em{isomorphic}, because the choice of constraint functions makes an important difference to the theory of optimization (for example, it can lead to different dual problems). Here we take the viewpoint that the \em{Lagrangian} is really the fundamental object in convex optimization - by passing to a suitable category of Lagrangians, we can make the dual problem into an actual self-duality on this category.
  }
}

\subtree{
  \title{Convex spaces}
  \transclude{efr-0020}

  \transclude{efr-002N}
  \transclude{efr-002M}
  \p{
    The term "convex function" in this sense clashes with the usual practice of naming structure-preserving functions after the structure they preserve (since convex functions do not preserve the convex structure). Unfortunately this usage is far too established to alter. (Convex functions are called convex because they are exactly those functions where the area above their graph is a convex subset of #{X \times \RR}. Although there appears to be no particular reason why the terms convex and concave should not be interchanged, other than convention).
  }
  \transclude{efr-002R}
  \transclude{efr-002Q}
  \transclude{efr-002O}
  \p{
    Justified by \ref{efr-002O}, we will appropriate the term \em{affine} to refer to #{\Delta}-homomorphisms, even between convex spaces which are not vector spaces. There is generally no chance of confusion, but it's worth emphasizing that the use of this term does not entail that the domain is closed under arbitrary affine combinations, for example.
  }
  \p{Convex spaces admit both a Cartesian product (given by the product of the underlying sets equipped with pointwise operations) and a tensor product, which (co)represents "bihomomorphisms". This is analogous to the situation for vector spaces. Unlike vector spaces, however, since all constant maps are homomorphisms, the projections #{X \times Y \to X,Y} are bihomomorphisms, which induces a map #{X \otimes Y \to X \times Y}. Thus homomorphisms #{X \times Y \to Z} are a subset of bihomomorphisms.}

  \p{Since we are generally dealing with convex or concave functions, which can't freely be extended to the tensor product, we will work with the Cartesian product in this paper. But it's very possible that most of our constructions would work also with the tensor product, and maybe there is some situation where the extra generality is necessary.}

  \transclude{efr-002S}

  \transclude{efr-002U}
}

\subtree{
  \title{The Category of Minmax problems}
  %def: minmax
  \transclude{efr-001C}

  \p{We will see that various constructions on this category, which are natural and well-behaved from the point of view of category theory, capture relevant constructions from the theory of convex optimization.}

  \ol{
    \li{#{\Minmax} is bifibred over #{\Set^\Delta \times \Set^{\Delta,\op}}, and the Cartesian and coCartesian lifts capture the operations of minimizing over the primal variables or maximizing over the dual variables}
    \li{The property of \em{strong duality} amounts to the claim that a particular diagram has the local Beck-Chevalley property}
    \li{Relatedly, the existence of a Nash equilibrium for the game corresponding to #{L} amounts to the existence of a certain morphism. The fact that this implies strong duality can be derived by purely categorical means.}
  }

  %Relation to ordinary ones
  \transclude{efr-002H}
  \p{
    Observe that minmax problems affine in #{A} are thus very similar to standard-form convex optimization problems, the main difference being that the set of allowed points in #{A} may be constrained in some other way than by requiring certain coordinates to be nonnegative.
  }
  \p{
    On the other hand, if #{A} is thus constrained, the proposition doesn't actually imply that #{f,g} are convex! The easiest way to see this is by considering #{A = \{a\}} for some nonzero #{a}. Then we have #{L(x,a) = f(x) + ag(x),}, and clearly we can choose this decomposition in such a way that these functions are not convex.
  }
  \p{
    However, if #{A \subset \RR^m} contains the positive cone #{\RR^m_+,} for example, we do have both #{f} and all the coordinates of #{g} convex.
  }

  %Primal dual problem
  \transclude{efr-001D}

  % Duality

  \transclude{efr-001J}

  \p{
    We will often utilize this duality to abbreviate proofs, proving something, for example, for the forwards direction and arguing "by duality" that it holds for the backwards direction as well.
  }


  %backwards/forwards maps
  \transclude{efr-0027}


  %fibration structure (min-maxing)
  
  \transclude{efr-002A}

  \transclude{efr-0023}

  \p{
    What's "really" going on here is that #{\Minmax} is a two-sided fibration, the result of taking the functor #{\Set^{\Delta,\op} \times \Set^{\Delta,\op} \to \Cat} carrying a pair #{X,Y} to the poset of minmax problems #{L: X \times Y \to \RR} (in the opposite order), with morphisms acting by precomposition, and applying the Grothendieck construction "contravariantly in the first variable and covariantly in the second variable". (And then observing that the precomposition action has left/right adjoints given by #{\inf}/#{\sup}, to make this into a \em{bi}fibration). But the theory of bifibrations is quite complicated in general, and we will not go into it here.
  }

  \p{
    Note also that this functor is quite close to displaying #{\Minmax} as \em{topological}. If we remove the restriction that minmax problems be convex/concave, we can construct the universal lifts required using a similar supremum formula. The problem is that the supremum of a general set of concave functions is not automatically concave (however, the supremum taken over a convex set, in a suitable sense, is).
  }
  

  %Conv/Conc
  \transclude{efr-001R}
  \transclude{efr-001S}
  \p{
    Note that if #{\phi = (\phi^+,\phi^-): L \to L'} is a morphism of #{\Minmax}, the two meanings of the notation #{\phi^+} agree, and the same is true of #{\phi^-}.
  }
  \p{
    Note also that the reflexive subcategory #{\Conc^\op \subseteq \Minmax} is the local subcategory with respect to the forwards morphisms - a morphism is forward if and only if #{\phi^-} is an isomorhism (by definition), and the unit #{L \to L^-} is the terminal forwards morphism with domain #{L}. A dual statement holds for #{\Conv \subseteq \Minmax} (it is the \em{colocalization} with respect to the class of backwards morphisms).
  }

  %monoidal structure
  \transclude{efr-001N}
  %monoidal fibration
  \transclude{efr-002D}
  \p{In the case of a Cartesian base, a monoidal fibration (like the one we have here) is equivalent to a fibration with a monoidal structure on each fiber, compatible with the reindexing in a certain way. Our base is not Cartesian, but does seem to come from a monoidal structure on each fiber, given by addition of #{L}s. The point is that, given #{(X,A,L)}, there is a canonical way to obtain an #{L} on #{(X\times Y, A \times B)}, given by using a Cartesian lift of #{X \times Y \to X} and a \em{coCartesian} lift of #{A \times B \to A}. This suggests there should be a useful theory of \em{monoidal two-sided fibrations}, but this notion does not appear to have been studied before.}
  
  %monoidal localization
  \transclude{efr-002P}
}



\subtree{
  \title{Strong duality}
  \transclude{efr-001W}

  %Definition:
  \transclude{efr-002F}

  %Equilibrium/state formulation
  \transclude{efr-001X}

  \p{
    If a minmax problem is a zero-sum game, a point #{I \to L \otimes L^*} is a choice of Nash equilibrium for this game.
  }

  %Beck-Chevalley
  \transclude{efr-0029}

  %Slater, classical
  \transclude{efr-002L}

  %Minimax theorem
  \transclude{efr-001E}
  \p{
        This theorem can be derived from the Kakutani fixpoint theorem in a very similar way to the usual proof of Nash's theorem about general, non-zerosum games - although note that it is not a special case, since #{X} and #{A} may not be simplices, and the payoff function here is merely convex, not necessarily \em{affine} as it is for a game-theoretic game.
  }
  \p{
    However, we will give a different proof, which uses the structure of #{\Minmax} in a more direct way. Essentially, we will use compactness to reduce to the case of simplexes, then use an inductive argument to reduce to the case where #{X = A = \Delta^1 = [0,1],} which can be shown by a direct topological argument. The inductive step is a fiber sequence argument, where we use the characterization of strong duality in terms of the Beck-Chevalley property, \ref{efr-0029}.
  }
  %
  \transclude{efr-002T}
  \transclude{efr-002V}
  \transclude{efr-002W}
  \transclude{efr-002X}
  \transclude{efr-002Y}
  
  \subtree{
    \title{Proof of \ref{efr-001E}}
    \p{
      By \ref{efr-002Y}, the pair #{(X,A)} is solvable, and strong duality holds. Since #{X,A} are both compact, there must exist #{x^*,a^*} attaining the infimum #{\inf_x \sup_a L(x,a)} and the supremum #{\sup_a \inf_x L(x,a)}. These form an equilibrium.
    }
  }


  \p{
    It is interesting to note the use of compactness here. Recall that topological compactness is closely connected with the property, also called compactness, of #{\Hom(X,-)} preserving filtered colimits (this property, instantiated in #{\Top}, is not actually the same thing as topological compactness). Our use of compactness here, to derive from the existence of a state in the "finitary" subproblems #{(L,X,\Delta^n)} the existence of a state in the entire problem, does not have this form (nor is it even the case that #{A} is the colimit of its subsimplices), but it's possible that the proof could be rewritten to make this step more categorical.
  }
  \p{
    The idea of proceeding by induction on #{n} was inspired by \ref{weinstein-elementary-minimax-2022}, although our proof is rather different - they are only looking at \em{affine} games, and hence their induction step is completely different (and they have no need for the complicated #{n=1} base case that we do), and since we are not merely interested in games on simplices, we need an additional compactness argument.
  }

  \p{We can use the minimax theorem to derive other statements of interest about convex optimization}
  \transclude{efr-002Z}
  \transclude{efr-0030}
}

\subtree{
  \title{The Legendre Transform}
  
  \transclude{efr-001P}
  \p{
    The convex conjugate is also called the \em{Legendre transform} or the \em{Fenchel-Legendre transform}. It is intimately related to convex duality. We will prove the following fundamental property of the convex conjugate using the categorical language of minmax problems, and along the way we will see the role that convex duality plays. Note that our invocation of the term "strong duality" here is somewhat more complicated than strictly necessary - normally one would merely invoke the separating hyperplane theorem directly.
  }

  %Proposition: dual of dual is id
  \transclude{efr-001Q}

  %defns of restriction to zero
  \transclude{efr-001T}

  %strong duality in restriction
  \transclude{efr-001U}

  \transclude{efr-003C}

  \transclude{efr-001V}


}


\subtree{
  \title{Misc stuff (sorting)}
  \transclude{efr-001Y}
  \transclude{efr-001M}
  %def: Lagrangian
  \transclude{efr-001K}
}